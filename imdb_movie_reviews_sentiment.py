# -*- coding: utf-8 -*-
"""IMDB_Movie_Reviews_Sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LR4G3WQoy0zfJmI2LZH6cRe8EBm67tsX
"""

# Installing necessary libraries
!pip install -q wordcloud emoji seaborn matplotlib

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
from wordcloud import WordCloud
import emoji
from sklearn.feature_extraction.text import CountVectorizer

# Set visualization style
sns.set_theme(style="whitegrid")

from google.colab import files
uploaded = files.upload()

# Loading the dataset into pandas DataFrame
df = pd.read_csv('IMDB_Dataset.csv')

"""## ***LLM***

# **DISTIL-BERT**

### **Step1: Load the necessary Libraries**
"""

import torch
import pandas as pd
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from torch.cuda.amp import autocast, GradScaler

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""### **Step2: Pre-processing the data**"""

df["sentiment_label"] = df["sentiment"].map({"negative": 0, "neutral": 1, "positive": 2})

df.shape

df.head(5)

df_sample = df.sample(5000)

"""### **Step3: Split Data into Training and Testing Sets**"""

train_texts, test_texts, train_labels, test_labels = train_test_split(
df_sample["review"].tolist(), df_sample["sentiment_label"].tolist(), test_size=0.2, random_state=42)

"""### **Step4: Load DistilBERT Tokenizer**"""

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

"""### **Step5: Define a PyTorch Dataset Class**"""

class IMDBDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long),
        }

"""### **Step6: Create DataLoaders**"""

train_dataset = IMDBDataset(train_texts, train_labels, tokenizer)
test_dataset = IMDBDataset(test_texts, test_labels, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)

"""### **Step7: Load Pretrained DistilBERT Model**"""

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)
model.to(device)

"""### **Step8: Define Optimizer and Loss Function**"""

optimizer = AdamW(model.parameters(), lr=2e-5)
criterion = torch.nn.CrossEntropyLoss()
scaler = GradScaler()  # Mixed precision training

"""### **Step9: Train the Model with Gradient Accumulation**"""

gradient_accumulation_steps = 4
num_epochs = 2

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for i, batch in enumerate(train_loader):
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        with autocast():
            outputs = model(input_ids, attention_mask=attention_mask)
            loss = criterion(outputs.logits, labels)

        loss = loss / gradient_accumulation_steps
        scaler.scale(loss).backward()

        if (i + 1) % gradient_accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1}: Loss = {total_loss / len(train_loader):.4f}")

# Save model
torch.save(model.state_dict(), "distilbert_imdb_model.pth")

"""# **BERT-base Uncased**"""

pip install transformers datasets torch scikit-learn pandas numpy

"""### **Step 1: Install Required Libraries**"""

import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer

"""### **Step 2: Load and Preprocess the Dataset**"""

# Convert sentiment labels to numeric values
label_mapping = {"negative": 0, "neutral": 1, "positive": 2}
df["sentiment_label"] = df["sentiment"].map(label_mapping)

print("Dataset Overview:")
print(df.head())

# Split into training and test sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df["review"].tolist(), df["sentiment_label"].tolist(), test_size=0.2, random_state=42
)

# Load BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

"""### **Step 3: Tokenizing the Text Data**"""

# Tokenize the text data
def tokenize_function(texts):
    return tokenizer(
        texts, padding="max_length", truncation=True, max_length=256, return_tensors="pt"
    )

train_encodings = tokenize_function(train_texts)
test_encodings = tokenize_function(test_texts)

"""### **Step 4: Convert Data to PyTorch Format**"""

# Convert labels to tensor format
train_labels = torch.tensor(train_labels)
test_labels = torch.tensor(test_labels)

"""### **Step 5: Create a PyTorch Dataset and DataLoader**"""

from torch.utils.data import Dataset, DataLoader

# Define a dataset class
class IMDBDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            key: tensor[idx] for key, tensor in self.encodings.items()
        } | {"labels": self.labels[idx]}

# Create dataset objects
train_dataset = IMDBDataset(train_encodings, train_labels)
test_dataset = IMDBDataset(test_encodings, test_labels)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4)

"""### **Step 6: Load BERT for Multi-Class Classification**"""

from transformers import AutoModelForSequenceClassification

# Load BERT model with a classification head
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

"""### **Step 7: Define Optimizer and Loss Function**"""

from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

"""### **Step 8: Train the Model**"""

from torch.cuda.amp import autocast, GradScaler

gradient_accumulation_steps = 4
scaler = GradScaler()

for epoch in range(3):
    model.train()
    total_loss = 0

    for i, batch in enumerate(train_loader):
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        with autocast():  # Enable mixed precision
            outputs = model(input_ids, attention_mask=attention_mask)
            loss = criterion(outputs.logits, labels)

        loss = loss / gradient_accumulation_steps  # Scale loss
        scaler.scale(loss).backward()  # Use mixed precision

        if (i + 1) % gradient_accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1}: Loss = {total_loss / len(train_loader):.4f}")

"""### **Step 9: Evaluate Model**"""

from sklearn.metrics import accuracy_score

model.eval()
predictions, true_labels = [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(true_labels, predictions)
print(f"Test Accuracy: {accuracy:.4f}")

"""### **Step 10: Save the Model**"""

print("\nDataset Info:")
print(df.info())





















# Check for missing values
missing_values = df.isnull().sum()
print("Missing Values:\n", missing_values)

df.describe().style.background_gradient(cmap='tab20c')

df['sentiment_num'] = [1 if sentiment == 'positive' else 0 for sentiment in df['sentiment']]
df.head()

"""### ***Bag of Words***"""

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

from tqdm import tqdm
corpus = []
for i in tqdm(range(0,len(df))):
    sentence = re.sub('[^a-zA-Z]',' ',df['review'][i]) ## each review is each sentence
    sentence = sentence.lower() ## Lower casing the words in each sentence
    sentence = sentence.split() ## splitting sentences to words and storing it as a list of words
    sentence = [ps.stem(word) for word in sentence if not word in stopwords.words('english')]  ## Removing stop words and applying stemming
    sentence = ' '.join(sentence)  ## Joining words again to form the sentences
    corpus.append(sentence) ### storing each sentences to corpus

ps.stem('running')

## Creating bag of words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=2500)
X1 = cv.fit_transform(corpus).toarray()

y1 = pd.get_dummies(df['sentiment'])
y1 = y1.iloc[:,1].values

from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1,y1,test_size = 0.20, random_state=0)

X_train1.shape

y_train1.shape

from sklearn.naive_bayes import MultinomialNB
model1 = MultinomialNB().fit(X_train1,y_train1)

y_pred1 = model1.predict(X_test1)

from sklearn.metrics import accuracy_score, classification_report
print(accuracy_score(y_test1,y_pred1))
print(classification_report(y_pred1,y_test1))

new_review = 'I love this movie so much. It\'s really great'
new_review = re.sub('[^a-zA-Z]', ' ', new_review)
new_review = new_review.lower()
new_review = new_review.split()
ps = PorterStemmer()
all_stopwords = stopwords.words('english')
all_stopwords.remove('not')
new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]
new_review = ' '.join(new_review)
new_corpus = [new_review]
new_X_test = cv.transform(new_corpus).toarray()
new_y_pred = model1.predict(new_X_test)
print(new_y_pred)

"""## ***TF-IDF***"""

## Creating tf-idf model
from sklearn.feature_extraction.text import TfidfVectorizer
tv = TfidfVectorizer(max_features=2500)
X2 = tv.fit_transform(corpus).toarray()

y2=pd.get_dummies(df['sentiment'])
y2=y2.iloc[:,1].values

from sklearn.model_selection import train_test_split
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size=0.20, random_state=0)

X_train2.shape

y_train2.shape

from sklearn.naive_bayes import MultinomialNB
model2 = MultinomialNB().fit(X_train2, y_train2)

y_pred2 = model2.predict(X_test2)

print(accuracy_score(y_test2,y_pred2))
print(classification_report(y_pred2,y_test2))

new_review = 'I Hate this movie so much. It\'s ok.'
new_review = re.sub('[^a-zA-Z]', ' ', new_review)
new_review = new_review.lower()
new_review = new_review.split()
ps = PorterStemmer()
all_stopwords = stopwords.words('english')
all_stopwords.remove('not')
new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]
new_review = ' '.join(new_review)
new_corpus = [new_review]
new_X_test = cv.transform(new_corpus).toarray()
new_y_pred = model2.predict(new_X_test)
print(new_y_pred)

"""# ***Word2Vec using RF***"""

import re
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split

# Download stopwords and lemmatizer if not already downloaded
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Preprocessing function
def preprocess_text(text):
    # 1. Convert to lowercase
    text = text.lower()

    # 2. Remove HTML tags
    text = re.sub(r'<.*?>', '', text)

    # 3. Remove punctuation and numbers
    text = re.sub(r'[^\w\s]', '', text)  # Removes punctuation
    text = re.sub(r'\d+', '', text)     # Removes numbers

    # 4. Tokenize (split text into words)
    tokens = text.split()

    # 5. Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]

    # 6. Lemmatize words
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back into a single string
    return ' '.join(tokens)

# Apply preprocessing to the review column
df['review_clean'] = df['review'].apply(preprocess_text)

df.head(5)

import os
print(os.getcwd())

import os
os.listdir('/content')

import nltk
nltk.download('punkt')  # Correctly download 'punkt' tokenizer

import nltk
print(nltk.data.find('tokenizers/punkt'))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import numpy as np
import joblib  # Import joblib for saving the model
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import re

# Assuming df contains the preprocessed text ('review_clean') and sentiment ('sentiment')
X = df['review_clean']
y = df['sentiment']

def tokenize(text):
    # Remove anything that's not a letter and lowercase the text
    return word_tokenize(re.sub('[^a-zA-Z]', ' ', text.lower()))  # Make sure punkt is loaded first

import nltk
nltk.data.path.append('/usr/local/share/nltk_data')

import spacy
nlp = spacy.load('en_core_web_sm')

def tokenize_spacy(text):
    doc = nlp(text)
    return [token.text for token in doc if token.is_alpha]  # Only keep alphabetic tokens

# Apply spacy tokenization to the 'review_clean' column
X_tokenized = X.apply(tokenize_spacy)

# Train a Word2Vec model on the tokenized reviews (if you don't have a pre-trained one)
word2vec_model = Word2Vec(sentences=X_tokenized, vector_size=100, window=5, min_count=1, workers=4)

# Save the Word2Vec model (optional)
word2vec_model.save('word2vec_model')  # Save the model to disk

# Function to average Word2Vec embeddings for each review
def get_average_word2vec(review, model, vector_size=100):
    # Get Word2Vec vectors for words in review
    words = review
    word_vecs = [model.wv[word] for word in words if word in model.wv]
    if len(word_vecs) == 0:
        return np.zeros(vector_size)  # Return zero vector if no words match in Word2Vec
    return np.mean(word_vecs, axis=0)

# Convert tokenized reviews to average Word2Vec embeddings
X_word2vec = np.array([get_average_word2vec(review, word2vec_model) for review in X_tokenized])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)

# Train RandomForest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the model
y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred))

# Save the RandomForest model
joblib.dump(rf_model, 'random_forest_model_word2vec.pkl')  # Save the trained RandomForest model
joblib.dump(word2vec_model, 'word2vec_model.pkl')  # Save the Word2Vec model (if needed)

# Sample new review to test
new_review = "This movie was absolutely amazing!"

# Tokenize and process the new review (using Spacy)
new_review_tokenized = tokenize_spacy(new_review)

# Get average Word2Vec embeddings for the new review
new_review_vector = get_average_word2vec(new_review_tokenized, word2vec_model)

# Reshape the vector to match the expected input format for RandomForest
new_review_vector = new_review_vector.reshape(1, -1)

# Predict sentiment
new_y_pred = rf_model.predict(new_review_vector)
print("Predicted sentiment:", new_y_pred)

# Sample new review to test
new_review = "The movie was very pathetic!"

# Tokenize and process the new review (using Spacy)
new_review_tokenized = tokenize_spacy(new_review)

# Get average Word2Vec embeddings for the new review
new_review_vector = get_average_word2vec(new_review_tokenized, word2vec_model)

# Reshape the vector to match the expected input format for RandomForest
new_review_vector = new_review_vector.reshape(1, -1)

# Predict sentiment
new_y_pred = rf_model.predict(new_review_vector)
print("Predicted sentiment:", new_y_pred)

# Sample new review to test
new_review = "Movie could have been better!"

# Tokenize and process the new review (using Spacy)
new_review_tokenized = tokenize_spacy(new_review)

# Get average Word2Vec embeddings for the new review
new_review_vector = get_average_word2vec(new_review_tokenized, word2vec_model)

# Reshape the vector to match the expected input format for RandomForest
new_review_vector = new_review_vector.reshape(1, -1)

# Predict sentiment
new_y_pred = rf_model.predict(new_review_vector)
print("Predicted sentiment:", new_y_pred)

"""## ***Word2Vec using GaussianNB***"""

# Required Libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report
from gensim.models import Word2Vec
import spacy

# Load SpaCy for Tokenization
nlp = spacy.load('en_core_web_sm')

# Tokenize Function (using SpaCy)
def tokenize(text):
    doc = nlp(text.lower())
    return [token.text for token in doc if token.is_alpha]  # Keep only alphabetic words

# Tokenize the reviews
df['tokenized'] = df['review'].apply(tokenize)

# Train Word2Vec Model
word2vec_model = Word2Vec(sentences=df['tokenized'], vector_size=100, window=5, min_count=1, workers=4)

# Save Word2Vec Model (Optional)
word2vec_model.save('word2vec_model')

# Function to Average Word2Vec Embeddings
def get_average_word2vec(tokens, model, vector_size=100):
    word_vecs = [model.wv[word] for word in tokens if word in model.wv]
    if len(word_vecs) == 0:
        return np.zeros(vector_size)  # If no words in vocab, return a zero vector
    return np.mean(word_vecs, axis=0)

# Convert Tokenized Reviews to Word2Vec Embeddings
df['w2v_features'] = df['tokenized'].apply(lambda tokens: get_average_word2vec(tokens, word2vec_model))

# Prepare Features and Labels
X = np.vstack(df['w2v_features'].values)  # Convert list of arrays to 2D array
y = df['sentiment']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and Train Gaussian Naive Bayes
gnb_model = GaussianNB()
gnb_model.fit(X_train, y_train)

# Predict and Evaluate
y_pred = gnb_model.predict(X_test)
print(classification_report(y_test, y_pred))

import joblib  # Import joblib for saving the model
# Save the GNB model
joblib.dump(gnb_model, 'GNB_model_word2vec.pkl')  # Save the trained RandomForest model
joblib.dump(word2vec_model, 'word2vec_model.pkl')  # Save the Word2Vec model (if needed)

import numpy as np
from gensim.models import Word2Vec
import joblib
import re

# Function to preprocess and tokenize the review
def tokenize(text):
    # Remove non-alphabet characters and split the text into words
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove anything that isn't a letter or space
    text = text.lower()  # Convert to lowercase
    return text.split()  # Split into words

# Function to process and convert the review to Word2Vec embedding
def get_average_word2vec(review, model, vector_size=100):
    words = review
    word_vecs = [model.wv[word] for word in words if word in model.wv]
    if len(word_vecs) == 0:
        return np.zeros(vector_size)
    return np.mean(word_vecs, axis=0)

# Load the pre-trained models
word2vec_model = Word2Vec.load('word2vec_model')  # Load Word2Vec model using gensim
gnb_model = joblib.load('GNB_model_word2vec.pkl')  # Load Naive Bayes model using joblib

# Example new review
new_review = 'I really enjoyed this movie. It was so good!'

# Tokenize the new review (using our custom tokenizer)
new_review_tokenized = tokenize(new_review)

# Convert the new review to its average Word2Vec embedding
new_review_vector = get_average_word2vec(new_review_tokenized, word2vec_model)

# Predict the sentiment using the trained Gaussian Naive Bayes (GNB) model
new_prediction = gnb_model.predict([new_review_vector])

# Print the predicted sentiment
print(f"Predicted Sentiment: {'Positive' if new_prediction[0] == 1 else 'Negative'}")

# Example new review
new_review = 'Movie is awesome!'

# Tokenize the new review (using our custom tokenizer)
new_review_tokenized = tokenize(new_review)

# Convert the new review to its average Word2Vec embedding
new_review_vector = get_average_word2vec(new_review_tokenized, word2vec_model)

# Predict the sentiment using the trained Gaussian Naive Bayes (GNB) model
new_prediction = gnb_model.predict([new_review_vector])

# Print the predicted sentiment
print(f"Predicted Sentiment: {'Positive' if new_prediction[0] == 1 else 'Negative'}")

# Example new review
new_review = 'Movie is worst!'

# Tokenize the new review (using our custom tokenizer)
new_review_tokenized = tokenize(new_review)

# Convert the new review to its average Word2Vec embedding
new_review_vector = get_average_word2vec(new_review_tokenized, word2vec_model)

# Predict the sentiment using the trained Gaussian Naive Bayes (GNB) model
new_prediction = gnb_model.predict([new_review_vector])

# Print the predicted sentiment
print(f"Predicted Sentiment: {'Positive' if new_prediction[0] == 1 else 'Negative'}")